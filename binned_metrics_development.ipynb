{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Binned Metrics Monitor for ModelOp Center\n",
    "\n",
    "This notebook is a \"development sandbox\" for creating and testing your `binned_metrics` monitor *before* you deploy it to ModelOp Center (MOC).\n",
    "\n",
    "**The workflow is:**\n",
    "1.  **Load Functions:** We'll load all the helper functions and MOC-wrapper functions (like `init` and `metrics`) into the notebook's memory.\n",
    "2.  **Test Components:** We'll test the functions individually, from the \"core engine\" to the \"universal function,\" using small, hard-coded data.\n",
    "3.  **Simulate MOC:** We'll run a final simulation that mimics exactly what MOC does: call `init()` with parameters, then call `metrics()` with a DataFrame.\n",
    "4.  **Deploy:** Once you are confident the logic is correct and the final JSON output is perfect, you can copy the code from Cell 3 into your `binned_metrics.py` file for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load The Full Monitor Script\n",
    "\n",
    "The code cell below contains the *entire* Python script (`binned_metrics.py`). We run this cell **once** to load all the functions into our notebook's memory, including:\n",
    "\n",
    "* `_safe_divide()`\n",
    "* `_calculate_metrics()`\n",
    "* `_apply_metrics_to_bin()`\n",
    "* `calculate_binned_metrics()`\n",
    "* `init()`\n",
    "* `_format_df_for_timeline_graph()`\n",
    "* `metrics()`\n",
    "\n",
    "(Note: The `main()` function is *not* included, as we will be running our tests directly in the notebook cells.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- \n",
    "# --- 1. HELPER & CORE FUNCTIONS ---\n",
    "# --- --- --- --- --- --- --- --- --- --- \n",
    "\n",
    "def _safe_divide(numerator, denominator, nan_val=np.nan):\n",
    "    \"\"\"\n",
    "    Prevents division by zero or division involving NaN values.\n",
    "    \"\"\"\n",
    "    if denominator == 0 or np.isnan(denominator) or np.isnan(numerator):\n",
    "        return nan_val\n",
    "    return numerator / denominator\n",
    "\n",
    "def _calculate_metrics(y_true, y_pred, requested_metrics):\n",
    "    \"\"\"\n",
    "    Calculates all specified metrics from a confusion matrix for a given bin.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        for metric in requested_metrics:\n",
    "            results[metric] = np.nan\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    except ValueError:\n",
    "        for metric in requested_metrics:\n",
    "            results[metric] = np.nan\n",
    "        return results\n",
    "\n",
    "    available_metrics = {}\n",
    "    P = tp + fn\n",
    "    N = tn + fp\n",
    "    PP = tp + fp\n",
    "    PN = tn + fn\n",
    "    Total = P + N\n",
    "    \n",
    "    available_metrics['SEN'] = _safe_divide(tp, P) if set(['SEN','F1','INF','J','BM','PT','TS','CSI','DOR']).intersection(set(requested_metrics)) else None\n",
    "    available_metrics['SP'] = _safe_divide(tn, N) if set(['SP','INF','J','BM','PT','DOR']).intersection(set(requested_metrics)) else None\n",
    "    available_metrics['PPV'] = _safe_divide(tp, PP) if set(['PPV','F1','MK']).intersection(set(requested_metrics)) else None\n",
    "    available_metrics['NPV'] = _safe_divide(tn, PN) if set(['NPV','MK']).intersection(set(requested_metrics)) else None\n",
    "    available_metrics['FNR'] = _safe_divide(fn, P) if 'FNR' in requested_metrics else None\n",
    "    available_metrics['FPR'] = _safe_divide(fp, N) if 'FPR' in requested_metrics else None\n",
    "    available_metrics['FDR'] = _safe_divide(fp, PP) if 'FDR' in requested_metrics else None\n",
    "    available_metrics['FOR'] = _safe_divide(fn, PN) if 'FOR' in requested_metrics else None\n",
    "    available_metrics['ACC'] = _safe_divide(tp + tn, Total) if 'ACC' in requested_metrics else None\n",
    "    available_metrics['ERR'] = _safe_divide(fp + fn, Total) if 'ERR' in requested_metrics else None\n",
    "    \n",
    "    if 'F1' in requested_metrics and available_metrics['PPV'] is not None and available_metrics['SEN'] is not None:\n",
    "        precision = available_metrics['PPV'] \n",
    "        sensitivity = available_metrics['SEN'] \n",
    "        available_metrics['F1'] = _safe_divide(2 * precision * sensitivity, precision + sensitivity)\n",
    "    else:\n",
    "        available_metrics['F1'] = None\n",
    "    \n",
    "    available_metrics['PR'] = _safe_divide(P, Total) if 'PR' in requested_metrics else None\n",
    "    \n",
    "    if 'MCC' in requested_metrics:\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        available_metrics['MCC'] = _safe_divide(mcc_num, mcc_den)\n",
    "    else:\n",
    "        available_metrics['MCC'] = None\n",
    "\n",
    "    available_metrics['INF'] = available_metrics['SEN'] + available_metrics['SP'] - 1 if set(['INF','J','BM']).intersection(set(requested_metrics)) and available_metrics['SEN'] is not None and available_metrics['SP'] is not None else None\n",
    "    available_metrics['J'] = available_metrics['INF'] if 'J' in requested_metrics else None\n",
    "    available_metrics['BM'] = available_metrics['INF'] if 'BM' in requested_metrics else None\n",
    "    available_metrics['MK'] = available_metrics['PPV'] + available_metrics['NPV'] - 1 if 'MK' in requested_metrics and available_metrics['PPV'] is not None and available_metrics['NPV'] is not None else None\n",
    "    \n",
    "    if 'PT' in requested_metrics and available_metrics['SEN'] is not None and available_metrics['SP'] is not None:\n",
    "            pt_num = (np.sqrt(available_metrics['SEN'] * (1 - available_metrics['SP'])) + available_metrics['SP'] - 1)\n",
    "            pt_den = (available_metrics['SEN'] + available_metrics['SP'] - 1)\n",
    "            available_metrics['PT'] = _safe_divide(pt_num, pt_den)\n",
    "    else:\n",
    "        available_metrics['PT'] = None\n",
    "\n",
    "    available_metrics['TS'] = _safe_divide(tp, tp + fn + fp) if set(['TS','CSI']).intersection(set(requested_metrics)) else None\n",
    "    available_metrics['CSI'] = available_metrics['TS'] if 'CSI' in requested_metrics else None\n",
    "\n",
    "    if 'DOR' in requested_metrics:\n",
    "        if tp == 0 or tn == 0 or fp == 0 or fn == 0:\n",
    "            available_metrics['DOR'] = np.nan\n",
    "        else:\n",
    "            dor_num = (tp * tn)\n",
    "            dor_den = (fp * fn)\n",
    "            available_metrics['DOR'] = _safe_divide(dor_num, dor_den)\n",
    "\n",
    "    for metric in requested_metrics:\n",
    "        results[metric] = available_metrics.get(metric, np.nan)\n",
    "\n",
    "    return results\n",
    "\n",
    "def _apply_metrics_to_bin(df_binned, requested_metrics):\n",
    "    \"\"\"\n",
    "    Pandas .apply() helper function.\n",
    "    \"\"\"\n",
    "    if df_binned.empty:\n",
    "        return pd.Series(index=requested_metrics, data=np.nan, dtype=float)\n",
    "\n",
    "    metrics_dict = _calculate_metrics(df_binned['y_true'],\n",
    "                                      df_binned['y_pred'],\n",
    "                                      requested_metrics)\n",
    "    \n",
    "    return pd.Series(metrics_dict)\n",
    "\n",
    "def calculate_binned_metrics(df, timestamp_col, bins, \n",
    "                             label_col, label_true, label_false,\n",
    "                             score_col, score_true, score_false,\n",
    "                             metrics=None, numeric_aggregations=None, logger=None):\n",
    "    \"\"\"\n",
    "    Calculates specified performance metrics and/or numeric aggregations\n",
    "    over different time bins.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    log_func = logger.warning if logger else print\n",
    "    \n",
    "    if not metrics and not numeric_aggregations:\n",
    "        log_func(\"Warning: No 'metrics' or 'numeric_aggregations' specified. Returning empty results.\")\n",
    "        return {bin_freq: pd.DataFrame() for bin_freq in bins}\n",
    "\n",
    "    if metrics:\n",
    "        log_func(\"Setting up for performance metric calculation...\")\n",
    "        \n",
    "        if not label_col or not score_col:\n",
    "            log_func(\"Error: 'label_col' and 'score_col' are required when 'metrics' are requested.\")\n",
    "            raise ValueError(\"'label_col' and 'score_col' are required when 'metrics' are requested.\")\n",
    "\n",
    "        if label_col not in df_copy.columns:\n",
    "            log_func(f\"Error: Label column '{label_col}' not found in DataFrame.\")\n",
    "            raise KeyError(f\"Label column '{label_col}' not found in DataFrame columns: {list(df_copy.columns)}\")\n",
    "        \n",
    "        if score_col not in df_copy.columns:\n",
    "            log_func(f\"Error: Score column '{score_col}' not found in DataFrame.\")\n",
    "            raise KeyError(f\"Score column '{score_col}' not found in DataFrame columns: {list(df_copy.columns)}\")\n",
    "\n",
    "        log_func(f\"Mapping actual column '{label_col}' using '{label_true}': 1, '{label_false}': 0\")\n",
    "        df_copy['y_true'] = np.where(\n",
    "            df_copy[label_col] == label_true, \n",
    "            1, \n",
    "            np.where(df_copy[label_col] == label_false, 0, np.nan)\n",
    "        )\n",
    "        \n",
    "        log_func(f\"Mapping predicted column '{score_col}' using '{score_true}': 1, '{score_false}': 0\")\n",
    "        df_copy['y_pred'] = np.where(\n",
    "            df_copy[score_col] == score_true,\n",
    "            1,\n",
    "            np.where(df_copy[score_col] == score_false, 0, np.nan)\n",
    "        )\n",
    "\n",
    "        if df_copy['y_true'].isnull().any():\n",
    "            unmapped_vals = df_copy[df_copy['y_true'].isnull()][label_col].unique()\n",
    "            log_func(f\"Warning: Found unmapped values in actual column '{label_col}': {unmapped_vals}. \"\n",
    "                     f\"These rows will be ignored for metric calculations.\")\n",
    "        if df_copy['y_pred'].isnull().any():\n",
    "            unmapped_vals = df_copy[df_copy['y_pred'].isnull()][score_col].unique()\n",
    "            log_func(f\"Warning: Found unmapped values in predicted column '{score_col}': {unmapped_vals}. \"\n",
    "                     f\"These rows will be ignored for metric calculations.\")\n",
    "        \n",
    "        metrics_df_copy = df_copy.dropna(subset=['y_true', 'y_pred'])\n",
    "        \n",
    "        if metrics_df_copy.empty and not numeric_aggregations:\n",
    "            log_func(\"Warning: No valid data remaining after mapping for metrics. Returning empty results.\")\n",
    "            return {bin_freq: pd.DataFrame(columns=metrics) for bin_freq in bins}\n",
    "            \n",
    "        metrics_df_copy['y_true'] = metrics_df_copy['y_true'].astype(int)\n",
    "        metrics_df_copy['y_pred'] = metrics_df_copy['y_pred'].astype(int)\n",
    "    else:\n",
    "        metrics_df_copy = pd.DataFrame() \n",
    "\n",
    "    try:\n",
    "        log_func(f\"Converting timestamp column '{timestamp_col}' to datetime objects.\")\n",
    "        df_copy[timestamp_col] = pd.to_datetime(df_copy[timestamp_col])\n",
    "        df_copy = df_copy.set_index(timestamp_col)\n",
    "        \n",
    "        if not metrics_df_copy.empty:\n",
    "            metrics_df_copy[timestamp_col] = pd.to_datetime(metrics_df_copy[timestamp_col])\n",
    "            metrics_df_copy = metrics_df_copy.set_index(timestamp_col)\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_func(f\"Error: Could not parse timestamp column '{timestamp_col}'. Error: {e}\")\n",
    "        raise ValueError(f\"Could not parse timestamp column '{timestamp_col}'. \"\n",
    "                         f\"Ensure it is a valid datetime format. Error: {e}\")\n",
    "\n",
    "    log_func(f\"Starting calculation loop for bins: {bins}\")\n",
    "    all_binned_metrics = {}\n",
    "    \n",
    "    for bin_freq in bins:\n",
    "        bin_results_dfs = [] \n",
    "        \n",
    "        if metrics:\n",
    "            if metrics_df_copy.empty:\n",
    "                 log_func(f\"Warning: 'metrics' were requested but no valid data rows were found after mapping. Skipping performance metrics for bin '{bin_freq}'.\")\n",
    "            else:\n",
    "                try:\n",
    "                    log_func(f\"Resampling performance metrics for bin '{bin_freq}'...\")\n",
    "                    metrics_df = metrics_df_copy.resample(bin_freq).apply(\n",
    "                        _apply_metrics_to_bin, \n",
    "                        requested_metrics=metrics\n",
    "                    )\n",
    "                    metrics_df = metrics_df.dropna(how='all')\n",
    "                    \n",
    "                    if not metrics_df.empty:\n",
    "                        bin_results_dfs.append(metrics_df)\n",
    "                        \n",
    "                except ValueError as e:\n",
    "                    log_func(f\"Warning: Invalid bin frequency string '{bin_freq}' for metrics. Skipping. Error: {e}\")\n",
    "                except Exception as e:\n",
    "                    log_func(f\"An unexpected error occurred during metrics resampling with bin '{bin_freq}': {e}\")\n",
    "\n",
    "        if numeric_aggregations:\n",
    "            try:\n",
    "                valid_agg_cols = {col: agg for col, agg in numeric_aggregations.items() if col in df_copy.columns}\n",
    "                missing_cols = set(numeric_aggregations.keys()) - set(valid_agg_cols.keys())\n",
    "                \n",
    "                if missing_cols:\n",
    "                    log_func(f\"Warning: For bin '{bin_freq}', the following columns for numeric aggregation were not found and will be skipped: {missing_cols}\")\n",
    "\n",
    "                if valid_agg_cols:\n",
    "                    log_func(f\"Resampling numeric aggregations for bin '{bin_freq}': {valid_agg_cols}\")\n",
    "                    agg_df = df_copy.resample(bin_freq).agg(valid_agg_cols)\n",
    "                    agg_df = agg_df.dropna(how='all')\n",
    "                    if not agg_df.empty:\n",
    "                        bin_results_dfs.append(agg_df)\n",
    "                else:\n",
    "                    log_func(f\"Warning: No valid columns found for numeric aggregation for bin '{bin_freq}'.\")\n",
    "\n",
    "            except (AttributeError, TypeError) as e:\n",
    "                log_func(f\"Warning: Invalid aggregation function provided in 'numeric_aggregations' for bin '{bin_freq}'. Skipping. Error: {e}\")\n",
    "            except Exception as e:\n",
    "                log_func(f\"An unexpected error occurred during numeric aggregation with bin '{bin_freq}': {e}\")\n",
    "\n",
    "        if not bin_results_dfs:\n",
    "            all_binned_metrics[bin_freq] = pd.DataFrame()\n",
    "        else:\n",
    "            all_binned_metrics[bin_freq] = pd.concat(bin_results_dfs, axis=1)\n",
    "\n",
    "    log_func(\"Binned calculations complete.\")\n",
    "    return all_binned_metrics\n",
    "\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- \n",
    "# --- 2. MODELOP CENTER FUNCTIONS ---\n",
    "# --- --- --- --- --- --- --- --- --- --- \n",
    "\n",
    "# Set up a global logger for init() to use\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# modelop.init\n",
    "def init(init_param):\n",
    "    \"\"\"\n",
    "    ModelOp init function. Sets global variables from job parameters.\n",
    "    \"\"\"\n",
    "    global logger\n",
    "    logger.info(\"Initializing binned metrics job...\")\n",
    "\n",
    "    try:\n",
    "        job = json.loads(init_param.get(\"rawJson\", \"{}\"))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not parse 'rawJson' in init_param. Using all default values. Error: {e}\")\n",
    "        job = {}\n",
    "\n",
    "    global TIMESTAMP_COLUMN, LABEL_COLUMN_NAME, LABEL_FALSE_VALUE, \\\n",
    "           LABEL_TRUE_VALUE, SCORE_COLUMN_NAME, SCORE_FALSE_VALUE, \\\n",
    "           SCORE_TRUE_VALUE, METRICS_TO_CALC, BINS_TO_CALC, NUMERIC_AGGS_TO_CALC\n",
    "\n",
    "    job_params = job.get(\"jobParameters\", {})\n",
    "    \n",
    "    try:\n",
    "        TIMESTAMP_COLUMN = job_params[\"TIMESTAMP_COLUMN\"]\n",
    "        logger.info(f\"Loaded TIMESTAMP_COLUMN from job parameters: '{TIMESTAMP_COLUMN}'\")\n",
    "    except Exception:\n",
    "        TIMESTAMP_COLUMN = 'Date'\n",
    "        logger.info(f\"Using default TIMESTAMP_COLUMN: '{TIMESTAMP_COLUMN}'\")\n",
    "    \n",
    "    try:\n",
    "        LABEL_COLUMN_NAME = job_params[\"LABEL_COLUMN_NAME\"]\n",
    "        logger.info(f\"Loaded LABEL_COLUMN_NAME from job parameters: '{LABEL_COLUMN_NAME}'\")\n",
    "    except Exception:\n",
    "        LABEL_COLUMN_NAME = 'label'\n",
    "        logger.info(f\"Using default LABEL_COLUMN_NAME: '{LABEL_COLUMN_NAME}'\")\n",
    "\n",
    "    try:\n",
    "        LABEL_FALSE_VALUE = job_params[\"LABEL_FALSE_VALUE\"]\n",
    "        logger.info(f\"Loaded LABEL_FALSE_VALUE from job parameters: '{LABEL_FALSE_VALUE}'\")\n",
    "    except Exception:\n",
    "        LABEL_FALSE_VALUE = False\n",
    "        logger.info(f\"Using default LABEL_FALSE_VALUE: {LABEL_FALSE_VALUE}\")\n",
    "        \n",
    "    try:\n",
    "        LABEL_TRUE_VALUE = job_params[\"LABEL_TRUE_VALUE\"]\n",
    "        logger.info(f\"Loaded LABEL_TRUE_VALUE from job parameters: '{LABEL_TRUE_VALUE}'\")\n",
    "    except Exception:\n",
    "        LABEL_TRUE_VALUE = True\n",
    "        logger.info(f\"Using default LABEL_TRUE_VALUE: {LABEL_TRUE_VALUE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Local Development & Testing\n",
    "\n",
    "Now that all our functions are loaded (from running the cell above), we can test them interactively. This is the main development loop:\n",
    "\n",
    "1.  Write a small test (like in the cells below).\n",
    "2.  Run the test and check the output.\n",
    "3.  If it's wrong, modify the code in the large **Cell 3**.\n",
    "4.  Re-run **Cell 3** to load your changes.\n",
    "5.  Re-run your test cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Test the Core Engine (`_calculate_metrics`)\n",
    "\n",
    "This is the lowest-level unit test. It checks the raw math. Does the function correctly calculate metrics from two simple lists of 0s and 1s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Data ---\n",
    "# Simple, hand-calculated example\n",
    "# TN = 2 (idx 0, 5)\n",
    "# FP = 1 (idx 3)\n",
    "# FN = 1 (idx 2)\n",
    "# TP = 2 (idx 1, 4)\n",
    "y_true_test = [0, 1, 1, 0, 1, 0] # Actuals\n",
    "y_pred_test = [0, 1, 0, 1, 1, 0] # Predictions\n",
    "\n",
    "# --- Expected Results ---\n",
    "# P = 3 (TP+FN), N = 3 (TN+FP)\n",
    "# SEN (TP/P) = 2/3 = 0.666\n",
    "# SP (TN/N) = 2/3 = 0.666\n",
    "# ACC (TP+TN)/Total = (2+2)/6 = 4/6 = 0.666\n",
    "\n",
    "# --- Requested Metrics ---\n",
    "metrics_to_test = ['SEN', 'SP', 'ACC', 'F1']\n",
    "\n",
    "# --- Run Test ---\n",
    "print(\"Testing _calculate_metrics...\")\n",
    "results = _calculate_metrics(y_true_test, y_pred_test, metrics_to_test)\n",
    "\n",
    "# --- Check Output ---\n",
    "print(f\"Calculated Metrics:\\n{results}\")\n",
    "\n",
    "# MOC DEV CONTEXT:\n",
    "# If this test fails, something is wrong with your fundamental metric\n",
    "# calculations. Fix this function in Cell 3 before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Test the Universal Function (`calculate_binned_metrics`)\n",
    "\n",
    "This is the **most important test**. Here, we test the *entire* data processing pipeline:\n",
    "\n",
    "1.  Does it correctly find and use the timestamp column?\n",
    "2.  Does it correctly *map* your custom values (e.g., `True`, `'NO'`) to 0s and 1s? (This is where your original `KeyError` came from!)\n",
    "3.  Does it correctly handle `NaN`s (unmapped values)?\n",
    "4.  Does it correctly bin the data?\n",
    "5.  Does it correctly calculate aggregations (like `mean`)?\n",
    "\n",
    "We build a sample DataFrame just like the one MOC will provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Build Sample Data ---\n",
    "# This data simulates the 'baseline' DataFrame MOC will provide\n",
    "data = {\n",
    "    'Date': [\n",
    "        '2023-01-01', '2023-01-02', '2023-01-08', '2023-01-09', # Jan 2023\n",
    "        '2023-02-03', '2023-02-04', '2023-02-15', '2023-02-16', # Feb 2023\n",
    "        '2024-01-01', '2024-01-02', '2024-01-05'  # Jan 2024 (with an unmapped 'N/A')\n",
    "    ],\n",
    "    'ground_truth': [\n",
    "        True, False, True, False,  \n",
    "        True, False, True, False, \n",
    "        True, False, 'N/A'        \n",
    "    ],\n",
    "    'model_prediction': [\n",
    "        'YES', 'NO', 'NO', 'YES',   # Jan: 1 TP, 1 TN, 1 FN, 1 FP\n",
    "        'YES', 'NO', 'YES', 'NO',   # Feb: 2 TP, 2 TN\n",
    "        'YES', 'NO', 'YES'          # Jan 2024: 1 TP, 1 TN, 1 unmapped\n",
    "    ],\n",
    "    'loan_amount': [\n",
    "        1000, 2000, 1500, 3000,\n",
    "        500, 1000, 2500, 4000,\n",
    "        8000, 9000, 5000\n",
    "    ]\n",
    "}\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. Define Job Parameters (as variables) ---\n",
    "# These simulate the 'jobParameters' you'd set in the MOC UI\n",
    "p_timestamp_col = 'Date'\n",
    "p_label_col = 'ground_truth'\n",
    "p_label_true = True\n",
    "p_label_false = False\n",
    "p_score_col = 'model_prediction'\n",
    "p_score_true = 'YES'\n",
    "p_score_false = 'NO'\n",
    "p_metrics = ['SEN', 'SP', 'ACC', 'F1']\n",
    "p_bins = ['MS', 'YS'] # Monthly, Yearly\n",
    "p_aggs = {'loan_amount': 'mean'}\n",
    "\n",
    "# MOC DEV CONTEXT:\n",
    "# This is your main development loop! \n",
    "# Change the data, change the parameters, and re-run this cell\n",
    "# to test every edge case you can think of (e.g., empty bins,\n",
    "# unmapped values, all-positive data, etc.)\n",
    "\n",
    "# --- 3. Run Test ---\n",
    "print(\"Testing calculate_binned_metrics...\")\n",
    "binned_results = calculate_binned_metrics(\n",
    "    df=test_df,\n",
    "    timestamp_col=p_timestamp_col,\n",
    "    bins=p_bins,\n",
    "    label_col=p_label_col,\n",
    "    label_true=p_label_true,\n",
    "    label_false=p_label_false,\n",
    "    score_col=p_score_col,\n",
    "    score_true=p_score_true,\n",
    "    score_false=p_score_false,\n",
    "    metrics=p_metrics,\n",
    "    numeric_aggregations=p_aggs,\n",
    "    logger=logging.getLogger() # Use the notebook's logger\n",
    ")\n",
    "\n",
    "# --- 4. Check Output ---\n",
    "print(\"\\n--- Monthly Results (MS) ---\")\n",
    "print(binned_results['MS'])\n",
    "# Expected for 2023-01: SEN=0.5, SP=0.5, ACC=0.5, loan_amount=1875\n",
    "# Expected for 2023-02: SEN=1.0, SP=1.0, ACC=1.0, loan_amount=2000\n",
    "# Expected for 2024-01: SEN=1.0, SP=1.0, ACC=1.0, loan_amount=7333.33\n",
    "\n",
    "print(\"\\n--- Yearly Results (YS) ---\")\n",
    "print(binned_results['YS'])\n",
    "# Expected for 2023: SEN=0.75, SP=0.75, ACC=0.75, loan_amount=1937.5\n",
    "# Expected for 2024: SEN=1.0, SP=1.0, ACC=1.0, loan_amount=7333.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Simulate the Full ModelOp Center Call\n",
    "\n",
    "Finally, we simulate the *exact* process MOC uses. This test confirms:\n",
    "\n",
    "1.  Does your `init()` function correctly parse the `init_param` JSON?\n",
    "2.  Does it correctly set all the global variables?\n",
    "3.  Does your `metrics()` function correctly read those global variables and call `calculate_binned_metrics`?\n",
    "4.  Is the *final JSON output* correctly formatted for the MOC timeline graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Sample init_param ---\n",
    "# This JSON string is exactly what MOC provides to init()\n",
    "test_init_params = {\n",
    "    \"rawJson\": json.dumps({\n",
    "        \"jobParameters\": {\n",
    "            \"TIMESTAMP_COLUMN\": \"Date\",\n",
    "            \"LABEL_COLUMN_NAME\": \"ground_truth\",\n",
    "            \"LABEL_FALSE_VALUE\": False,\n",
    "            \"LABEL_TRUE_VALUE\": True,\n",
    "            \"SCORE_COLUMN_NAME\": \"model_prediction\",\n",
    "            \"SCORE_FALSE_VALUE\": \"NO\",\n",
    "            \"SCORE_TRUE_VALUE\": \"YES\",\n",
    "            \"METRICS_TO_CALC\": [\"SEN\", \"SP\", \"ACC\"],\n",
    "            \"BINS_TO_CALC\": [\"W\", \"MS\", \"YS\"],\n",
    "            \"NUMERIC_AGGS_TO_CALC\": {\"loan_amount\": \"mean\"}\n",
    "        }\n",
    "    })\n",
    "}\n",
    "\n",
    "# --- 2. Run init() ---\n",
    "# This will set the global variables inside the notebook's memory\n",
    "print(\"--- Calling init() ---\")\n",
    "init(test_init_params)\n",
    "print(\"Global variables set.\")\n",
    "\n",
    "# --- 3. Create baseline_df ---\n",
    "# We can re-use the same test_df from the previous step\n",
    "baseline_df = test_df.copy()\n",
    "print(f\"Using baseline_df with {len(baseline_df)} rows.\")\n",
    "\n",
    "# --- 4. Run metrics() ---\n",
    "# 'metrics' is a generator, so we use next() to get the first yield\n",
    "print(\"--- Calling metrics() ---\")\n",
    "final_moc_output = next(metrics(baseline_df))\n",
    "\n",
    "# --- 5. Check Final Output ---\n",
    "# This is the *exact* JSON that will be saved in MOC and\n",
    "# used to render the UI graphs.\n",
    "print(\"\\n--- FINAL MOC JSON OUTPUT ---\")\n",
    "print(json.dumps(final_moc_output, indent=2))\n",
    "\n",
    "# MOC DEV CONTEXT:\n",
    "# Check this JSON carefully.\n",
    "# - Are 'firstPredictionDate' and 'lastPredictionDate' correct?\n",
    "# - Do the graph titles look right?\n",
    "# - Is the 'data' key populated? (e.g., \"SEN\": [[\"2023-01-01\", 0.5]])\n",
    "# If this JSON is correct, your monitor is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "You have successfully tested the monitor at three levels:\n",
    "1.  **Unit Test** (`_calculate_metrics`): The core math is correct.\n",
    "2.  **Integration Test** (`calculate_binned_metrics`): The data processing, mapping, and binning logic is correct.\n",
    "3.  **Simulation Test** (`init` + `metrics`): The MOC-wrapper logic is correct, and the final JSON output is properly formatted.\n",
    "\n",
    "**To deploy:**\n",
    "1.  Make any final changes to the code in **Cell 3**.\n",
    "2.  Copy the *entire contents* of **Cell 3**.\n",
    "3.  Paste this code into your `binned_metrics.py` file.\n",
    "4.  Add this file to your ModelOp Center monitor repository along with your `readme.md` and `required_assets.json`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}